{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip install pandas_ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas_ta as ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bist_daily = pd.read_csv(\"datasets/bist_100_daily.csv\")\n",
    "# Extract the number of rows and columns by using the shape of the data.\n",
    "numRows,numColumns = bist_daily.shape\n",
    "# Extract the time interval.\n",
    "last_date, first_date = bist_daily.iloc[0].Date, bist_daily.iloc[-1].Date\n",
    "# Check the availability of the data.\n",
    "na_cols = bist_daily.columns[bist_daily.isna().any()].tolist()\n",
    "\n",
    "# Print the information.\n",
    "print(f\"There are {numRows} rows and {numColumns} columns in the initial dataset.\")\n",
    "print(f\"The data represents the time frame between the dates '{last_date}' and '{first_date}'.\")\n",
    "if not na_cols:\n",
    "    print(\"There are no NA rows.\")\n",
    "else:\n",
    "    print(f\"Columns in the dataset which include NA rows: {na_cols}.\")\n",
    "# Convert columns to numeric values\n",
    "column_names = [\"Price\", \"Open\", \"High\", \"Low\"]\n",
    "for column in column_names:\n",
    "    bist_daily[column] = bist_daily[column].str.replace(',', '')\n",
    "    bist_daily[column] = pd.to_numeric(bist_daily[column])\n",
    "# CONVERT TO DATETIME FORMAT AND SORT DATA BY DATE\n",
    "bist_daily.Date = pd.to_datetime(bist_daily.Date)\n",
    "bist_daily.sort_values(by=\"Date\", ignore_index=True,inplace=True)\n",
    "bist_daily.set_index(pd.DatetimeIndex(bist_daily[\"Date\"]), inplace=True)\n",
    "bist_daily.rename(columns={\"Price\": \"close\"},inplace=True)\n",
    "# Calculate Returns and append to the df DataFrame\n",
    "# CUMLOGRET_1 and CUMPCTRET_1 are added (NaN values exists)\n",
    "bist_daily.ta.log_return(cumulative=True, append=True)\n",
    "bist_daily.ta.percent_return(cumulative=True, append=True)\n",
    "# Returns a list of indicators and utility functions (to check in future)\n",
    "ind_list = bist_daily.ta.indicators(as_list=True)\n",
    "# RSI_14, MACD_12_26_9, MACDh_12_26_9 and MACDs_12_26_9 are added (NaN values exists)\n",
    "bist_daily.ta.rsi(append=True)\n",
    "bist_daily.ta.macd(append=True)\n",
    "# SMA values are added (use ta in the future)\n",
    "sma_values = [5, 10, 15] \n",
    "for i in sma_values:\n",
    "    bist_daily['SMA'+str(i)] = bist_daily['close'].rolling(window=i).mean()\n",
    "# Remove all NaN value rows\n",
    "bist_daily.dropna(inplace=True)\n",
    "bist_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# seeding an arbitrary number to get same results in multiple runs\n",
    "manualSeed = 999\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "print(\"Seed:\", manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# getting number of GPUs from cuda\n",
    "ngpu = torch.cuda.device_count()\n",
    "print(\"Count of available GPUs:\", ngpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# printing the name of available GPUs\n",
    "for i in range(ngpu):\n",
    "    print(\"GPU {}: {}\".format(i+1, torch.cuda.get_device_name(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# batch size for the training\n",
    "batch_size = 64\n",
    "\n",
    "# optimizer rates\n",
    "optimizer_betas = (0.9, 0.999)\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# number of epochs\n",
    "num_epochs = 100000\n",
    "\n",
    "# decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TimeseriesDataset(Dataset):\n",
    "    def __init__(self, data_frame, sequence_length=2):\n",
    "        self.data = torch.tensor(data_frame.values)\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0] - self.sequence_length + 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index: index + self.sequence_length].float()\n",
    "    \n",
    "    # Non-overlapping series\n",
    "    # def __getitem__(self, index):\n",
    "    #     return self.data[index * self.sequence_length: (index+1) * self.sequence_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create pytorch dataset from the pandas DataFrame\n",
    "\n",
    "# TODO: Convert change(%) and Volume columns to numeric values\n",
    "columns_used_in_training = [\"close\", \"open\", \"high\", \"low\", \"CUMLOGRET_1\", \"RSI_14\", \"MACD_12_26_9\", \"SMA5\"]\n",
    "# input dimension of the generator\n",
    "data_dimension = len(columns_used_in_training)\n",
    "# sequence length of input data\n",
    "sequence_length = 30\n",
    "train_dataset = TimeseriesDataset(bist_daily[columns_used_in_training], sequence_length)\n",
    "# create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "real_data_sample = next(iter(dataloader))\n",
    "print(\"Real data sample shape:\", real_data_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size=data_dimension, hidden_size=hidden_size, num_layers=2, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, data_dimension)\n",
    "        self.drop = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, input_sequences):\n",
    "        input_sequences = self.drop(input_sequences)\n",
    "        lstm_output, hidden_cell = self.lstm(input_sequences)\n",
    "        res = self.linear(hidden_cell[0][-1])\n",
    "        res = res.view(res.shape[0], 1, -1)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(sequence_length*data_dimension, 72),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(72, 100),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(100, 10),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(10, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_sequences):\n",
    "        input_sequences_flattened = input_sequences.view(input_sequences.shape[0], -1)\n",
    "        res = self.model(input_sequences_flattened)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: weight initialization of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generator = Generator(hidden_size=data_dimension*2).to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "print(\"Generator and discriminator are initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer_generator = optim.Adam(generator.parameters(), lr=learning_rate, betas=optimizer_betas)\n",
    "optimizer_discriminator = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=optimizer_betas)\n",
    "\n",
    "real_label = 1.\n",
    "fake_label = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Training is started\")\n",
    "for epoch in range(num_epochs):\n",
    "    for i, sequence_batch in enumerate(dataloader):\n",
    "            ############################\n",
    "            # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "            ###########################\n",
    "            ## Training with real batch\n",
    "            discriminator.zero_grad()\n",
    "            # Format batch\n",
    "            real_sequence = sequence_batch.to(device)\n",
    "            batch_size = real_sequence.size(0)\n",
    "            real_labels = torch.full((batch_size,), real_label, dtype=torch.float, device=device)\n",
    "            # Forward pass real batch through D\n",
    "            discriminator_output_real = discriminator(real_sequence).view(-1)\n",
    "            # Calculate loss on all-real batch\n",
    "            discriminator_error_real = criterion(discriminator_output_real, real_labels)\n",
    "            # Calculate gradients for D in backward pass\n",
    "            discriminator_error_real.backward()\n",
    "\n",
    "            ## Training with fake batch\n",
    "            # Assign first t values\n",
    "            generator_input_sequence = sequence_batch[:,:-1].to(device)\n",
    "            #  Generate (t+1)th value from first t values\n",
    "            generated_values = generator(generator_input_sequence)\n",
    "            fake_labels = torch.full((batch_size,), fake_label, dtype=torch.float, device=device)\n",
    "            # Concat first t real values and generated (t+1)th values\n",
    "            generator_result_concat = torch.cat((generator_input_sequence, generated_values.detach()), 1)\n",
    "            # Classify all fake batch with D\n",
    "            discriminator_output_fake = discriminator(generator_result_concat).view(-1)\n",
    "            # Calculate D's loss on the all-fake batch\n",
    "            discriminator_error_fake = criterion(discriminator_output_fake, fake_labels)\n",
    "            # Calculate the gradients for this batch\n",
    "            discriminator_error_fake.backward()\n",
    "            # Add the gradients from the all-real and all-fake batches\n",
    "            discriminator_error = discriminator_error_real + discriminator_error_fake\n",
    "            # Update D\n",
    "            optimizer_discriminator.step()\n",
    "\n",
    "            ############################\n",
    "            # (2) Update G network: maximize log(D(G(z)))\n",
    "            ###########################\n",
    "            generator.zero_grad()\n",
    "            real_labels = torch.full((batch_size,), real_label, dtype=torch.float, device=device)\n",
    "            # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "            generator_result_concat_grad = torch.cat((generator_input_sequence, generated_values), 1)\n",
    "            discriminator_output_fake = discriminator(generator_result_concat_grad).view(-1)\n",
    "            # Calculate G's loss based on this output\n",
    "            generator_error = criterion(discriminator_output_fake, real_labels)\n",
    "            # Calculate gradients for G\n",
    "            generator_error.backward()\n",
    "            # Update G\n",
    "            optimizer_generator.step()\n",
    "    if (epoch+1) % 50 == 0 or epoch+1 == 1:\n",
    "        print('\\n[{}/{}]\\tDiscriminator Loss: {:.4f}\\tGenerator Loss: {:.4f}'\n",
    "                  .format(epoch+1, num_epochs, discriminator_error.item(), generator_error.item()))\n",
    "        for col_name, real, generated in zip(columns_used_in_training, sequence_batch[0][-1], generated_values[0][0]):\n",
    "            print(f\"{col_name} | Real:{real:.4f} / Generated:{generated:.4f}\")\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
